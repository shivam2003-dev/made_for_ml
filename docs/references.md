# References

## Course Roadmap

???+ note "üó∫Ô∏è Learning Roadmap"

    This roadmap provides a suggested learning path through the course materials and references.

    ### Phase 1: Foundations (Weeks 1-3)
    
    **Objective**: Build strong theoretical and practical foundations
    
    1. **Start Here**:
       - Read Chapter 1: Introduction to Machine Learning
       - Review Chapter 2: Learning Paradigms
       - Study Mitchell (1997) - Chapters 1-2
    
    2. **Core Concepts**:
       - Chapter 3: Model Selection and Evaluation
       - Bishop (2006) - Chapter 1: Introduction
       - Hastie et al. (2009) - Chapter 2: Overview of Supervised Learning
    
    3. **Practice**:
       - Implement basic evaluation metrics
       - Practice with scikit-learn cross-validation
    
    ### Phase 2: Core Algorithms (Weeks 4-8)
    
    **Objective**: Master fundamental ML algorithms
    
    1. **Bayesian Methods** (Week 4):
       - Chapter 4: Bayesian Learning
       - Mitchell (1997) - Chapter 6
       - Bishop (2006) - Chapter 2: Probability Distributions
    
    2. **Linear Models** (Week 5):
       - Chapter 5: Linear Models
       - Bishop (2006) - Chapters 3-4
       - Implement Ridge and Lasso from scratch
    
    3. **Decision Trees** (Week 6):
       - Chapter 6: Decision Trees
       - Quinlan (1986) - ID3 paper
       - Implement decision tree with pruning
    
    4. **Instance-Based Learning** (Week 7):
       - Chapter 7: Instance-Based Learning
       - Cover & Hart (1967) - KNN foundational paper
       - Implement weighted KNN
    
    5. **Support Vector Machines** (Week 8):
       - Chapter 8: Support Vector Machines
       - Burges (1998) - SVM tutorial
       - Cortes & Vapnik (1995) - Original SVM paper
    
    ### Phase 3: Advanced Topics (Weeks 9-12)
    
    **Objective**: Explore advanced ML techniques
    
    1. **Neural Networks** (Weeks 9-10):
       - Chapter 9: Neural Networks
       - Rumelhart et al. (1986) - Backpropagation paper
       - Goodfellow et al. (2016) - Deep Learning book
       - Implement MLP with backpropagation
    
    2. **Genetic Algorithms** (Week 11):
       - Chapter 10: Genetic Algorithms
       - Holland (1975) - Original GA book
       - Implement GA for optimization
    
    3. **Integration & Projects** (Week 12):
       - Compare all algorithms on benchmark datasets
       - Implement ensemble methods
       - Work on course project
    
    ### Phase 4: Specialization (Weeks 13-16)
    
    **Objective**: Deep dive into areas of interest
    
    1. **Choose Specialization**:
       - Deep Learning (Goodfellow et al., 2016)
       - Reinforcement Learning (Sutton & Barto, 2018)
       - Statistical Learning Theory (Vapnik, 1998)
       - Bayesian Methods (Bishop, 2006)
    
    2. **Research Papers**:
       - Read recent papers from arXiv
       - Review foundational papers in your area
       - Implement and compare methods
    
    3. **Capstone Project**:
       - Apply learned techniques to real-world problem
       - Write comprehensive report
       - Present findings
    
    ### Recommended Study Strategy
    
    - **Daily**: 2-3 hours of study
    - **Weekly**: Complete one chapter + implement algorithms
    - **Bi-weekly**: Review and practice exercises
    - **Monthly**: Work on mini-projects
    
    ### Key Milestones
    
    - ‚úÖ Week 3: Complete foundations
    - ‚úÖ Week 8: Master core algorithms
    - ‚úÖ Week 12: Complete all chapters
    - ‚úÖ Week 16: Finish specialization project

## Primary Textbooks

???+ note "üìö T1: Machine Learning by Tom M. Mitchell"

    **Mitchell, T. M.** (1997). *Machine Learning*. The McGraw-Hill Companies, Inc. Indian Edition.

    - [Amazon](https://www.amazon.com/Machine-Learning-Tom-M-Mitchell/dp/0070428077)
    - [McGraw-Hill](https://www.mheducation.com/highered/product/machine-learning-mitchell/M9780070428072.html)
    - [Google Books](https://books.google.com/books/about/Machine_Learning.html?id=EoYBngEACAAJ)

    This is the primary textbook for the course, covering fundamental concepts in machine learning including:
    - Decision tree learning
    - Neural networks
    - Bayesian learning
    - Instance-based learning
    - Genetic algorithms

    **Key Chapters**:
    - Chapter 1: Introduction
    - Chapter 3: Decision Tree Learning
    - Chapter 4: Artificial Neural Networks
    - Chapter 6: Bayesian Learning
    - Chapter 8: Instance-Based Learning
    - Chapter 9: Genetic Algorithms

???+ note "üìö R1: Pattern Recognition & Machine Learning by Christopher M. Bishop"

    **Bishop, C. M.** (2006). *Pattern Recognition & Machine Learning*. Springer.

    - [Springer](https://link.springer.com/book/10.1007/978-0-387-45528-0)
    - [Amazon](https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738)
    - [Free PDF (Author's Website)](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)

    A comprehensive text with strong mathematical foundations, covering:
    - Linear models for regression and classification
    - Neural networks
    - Kernel methods
    - Bayesian approaches

    **Key Chapters**:
    - Chapter 1: Introduction
    - Chapter 3: Linear Models for Regression
    - Chapter 4: Linear Models for Classification
    - Chapter 5: Neural Networks
    - Chapter 7: Sparse Kernel Machines (SVMs)

???+ note "üìö R2: Introduction to Data Mining by Tan, Steinbach, and Kumar"

    **Tan, P. N., Steinbach, M., & Kumar, V.** (2019). *Introduction To Data Mining* (2nd Edition). Pearson.

    - [Pearson](https://www.pearson.com/en-us/subject-catalog/p/introduction-to-data-mining/P200000003431)
    - [Amazon](https://www.amazon.com/Introduction-Data-Mining-Pang-Ning/dp/0133128903)
    - [Author's Website](https://www-users.cse.umn.edu/~kumar001/dmbook/index.php)

    Focuses on practical data mining techniques and algorithms:
    - Classification
    - Clustering
    - Association analysis
    - Anomaly detection

## Additional References

???+ note "üìñ Support Vector Machines"

    **Burges, C. J. C.** (1998). A Tutorial on Support Vector Machines for Pattern Recognition. *Data Mining and Knowledge Discovery*, 2(2), 121-167.

    - [DOI: 10.1023/A:1009715923555](https://doi.org/10.1023/A:1009715923555)
    - [PDF](https://link.springer.com/article/10.1023/A:1009715923555)

    Comprehensive tutorial covering:
    - Mathematical foundations of SVMs
    - Kernel methods
    - Practical implementation considerations

### Research Papers

???+ note "üî¨ Bayesian Learning"

    1. **Domingos, P., & Pazzani, M.** (1997). On the optimality of the simple Bayesian classifier under zero-one loss. *Machine Learning*, 29(2-3), 103-130.

       - [DOI: 10.1023/A:1007413511361](https://doi.org/10.1023/A:1007413511361)
       - [PDF](https://link.springer.com/article/10.1023/A:1007413511361)

    2. **Rissanen, J.** (1978). Modeling by shortest data description. *Automatica*, 14(5), 465-471. (MDL Principle)

       - [DOI: 10.1016/0005-1098(78)90005-5](https://doi.org/10.1016/0005-1098(78)90005-5)
       - [PDF](https://www.sciencedirect.com/science/article/pii/0005109878900055)

???+ note "üî¨ Model Selection"

    3. **Geman, S., Bienenstock, E., & Doursat, R.** (1992). Neural networks and the bias/variance dilemma. *Neural Computation*, 4(1), 1-58.

       - [DOI: 10.1162/neco.1992.4.1.1](https://doi.org/10.1162/neco.1992.4.1.1)
       - [PDF](https://direct.mit.edu/neco/article/4/1/1/5515/Neural-Networks-and-the-Bias-Variance-Dilemma)

    4. **Kohavi, R.** (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. *IJCAI*, 14(2), 1137-1145.

       - [PDF](https://www.ijcai.org/Proceedings/95-2/Papers/016.pdf)
       - [CiteSeerX](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.48.529&rep=rep1&type=pdf)

???+ note "üî¨ Linear Models"

    5. **Tibshirani, R.** (1996). Regression shrinkage and selection via the lasso. *Journal of the Royal Statistical Society*, 58(1), 267-288.

       - [DOI: 10.1111/j.2517-6161.1996.tb02080.x](https://doi.org/10.1111/j.2517-6161.1996.tb02080.x)
       - [PDF](https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1996.tb02080.x)

    6. **Hoerl, A. E., & Kennard, R. W.** (1970). Ridge regression: Biased estimation for nonorthogonal problems. *Technometrics*, 12(1), 55-67.

       - [DOI: 10.1080/00401706.1970.10488634](https://doi.org/10.1080/00401706.1970.10488634)
       - [PDF](https://www.tandfonline.com/doi/pdf/10.1080/00401706.1970.10488634)

???+ note "üî¨ Decision Trees"

    7. **Quinlan, J. R.** (1986). Induction of decision trees. *Machine Learning*, 1(1), 81-106. (ID3 Algorithm)

       - [DOI: 10.1007/BF00116251](https://doi.org/10.1007/BF00116251)
       - [PDF](https://link.springer.com/article/10.1007/BF00116251)

    8. **Quinlan, J. R.** (1993). *C4.5: Programs for Machine Learning*. Morgan Kaufmann.

       - [Amazon](https://www.amazon.com/C4-5-Programs-Machine-Learning/dp/1558602380)
       - [Morgan Kaufmann](https://www.elsevier.com/books/c45-programs-for-machine-learning/quinlan/978-1-55860-238-0)

    9. **Breiman, L., Friedman, J., Stone, C. J., & Olshen, R. A.** (1984). *Classification and Regression Trees*. Wadsworth.

       - [Amazon](https://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418)
       - [Taylor & Francis](https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman)

???+ note "üî¨ Instance-Based Learning"

    10. **Cover, T., & Hart, P.** (1967). Nearest neighbor pattern classification. *IEEE Transactions on Information Theory*, 13(1), 21-27.

        - [DOI: 10.1109/TIT.1967.1053964](https://doi.org/10.1109/TIT.1967.1053964)
        - [IEEE Xplore](https://ieeexplore.ieee.org/document/1053964)

    11. **Aha, D. W., Kibler, D., & Albert, M. K.** (1991). Instance-based learning algorithms. *Machine Learning*, 6(1), 37-66.

        - [DOI: 10.1023/A:1022689900470](https://doi.org/10.1023/A:1022689900470)
        - [PDF](https://link.springer.com/article/10.1023/A:1022689900470)

???+ note "üî¨ Support Vector Machines"

    12. **Cortes, C., & Vapnik, V.** (1995). Support-vector networks. *Machine Learning*, 20(3), 273-297.

        - [DOI: 10.1007/BF00994018](https://doi.org/10.1007/BF00994018)
        - [PDF](https://link.springer.com/article/10.1007/BF00994018)

    13. **Vapnik, V. N.** (1998). *Statistical Learning Theory*. Wiley.

        - [Amazon](https://www.amazon.com/Statistical-Learning-Theory-Vladimir-Vapnik/dp/0471030031)
        - [Wiley](https://www.wiley.com/en-us/Statistical+Learning+Theory-p-9780471030034)

    14. **Vapnik, V., & Chervonenkis, A.** (1971). On the uniform convergence of relative frequencies of events to their probabilities. *Theory of Probability and its Applications*, 16(2), 264-280. (VC Dimension)

        - [DOI: 10.1137/1116025](https://doi.org/10.1137/1116025)
        - [PDF](https://epubs.siam.org/doi/pdf/10.1137/1116025)

???+ note "üî¨ Neural Networks"

    15. **Rumelhart, D. E., Hinton, G. E., & Williams, R. J.** (1986). Learning representations by back-propagating errors. *Nature*, 323(6088), 533-536.

        - [DOI: 10.1038/323533a0](https://doi.org/10.1038/323533a0)
        - [Nature](https://www.nature.com/articles/323533a0)
        - [PDF](https://www.nature.com/articles/323533a0.pdf)

    16. **Hornik, K., Stinchcombe, M., & White, H.** (1989). Multilayer feedforward networks are universal approximators. *Neural Networks*, 2(5), 359-366.

        - [DOI: 10.1016/0893-6080(89)90020-8](https://doi.org/10.1016/0893-6080(89)90020-8)
        - [PDF](https://www.sciencedirect.com/science/article/pii/0893608089900208)

    17. **Goodfellow, I., Bengio, Y., & Courville, A.** (2016). *Deep Learning*. MIT Press.

        - [MIT Press](https://mitpress.mit.edu/9780262035613/deep-learning/)
        - [Free Online Book](https://www.deeplearningbook.org/)
        - [Amazon](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618)

???+ note "üî¨ Genetic Algorithms"

    18. **Holland, J. H.** (1975). *Adaptation in Natural and Artificial Systems*. University of Michigan Press.

        - [Amazon](https://www.amazon.com/Adaptation-Natural-Artificial-Systems-Introductory/dp/0262581116)
        - [MIT Press (Reprint)](https://mitpress.mit.edu/9780262581110/adaptation-in-natural-and-artificial-systems/)

    19. **Goldberg, D. E.** (1989). *Genetic Algorithms in Search, Optimization, and Machine Learning*. Addison-Wesley.

        - [Amazon](https://www.amazon.com/Genetic-Algorithms-Search-Optimization-Machine/dp/0201157675)
        - [Pearson](https://www.pearson.com/en-us/subject-catalog/p/genetic-algorithms-in-search-optimization-and-machine-learning/P200000003431)

    20. **Mitchell, M.** (1998). *An Introduction to Genetic Algorithms*. MIT Press.

        - [MIT Press](https://mitpress.mit.edu/9780262631853/an-introduction-to-genetic-algorithms/)
        - [Amazon](https://www.amazon.com/Introduction-Genetic-Algorithms-Complex-Adaptive/dp/0262631857)

???+ note "üî¨ Statistical Learning Theory"

    21. **Hastie, T., Tibshirani, R., & Friedman, J.** (2009). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction* (2nd Edition). Springer.

        - [Springer](https://link.springer.com/book/10.1007/978-0-387-84858-7)
        - [Free PDF (Author's Website)](https://hastie.su.domains/ElemStatLearn/)
        - [Amazon](https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576)

    22. **Mohri, M., Rostamizadeh, A., & Talwalkar, A.** (2018). *Foundations of Machine Learning* (2nd Edition). MIT Press.

        - [MIT Press](https://mitpress.mit.edu/9780262039406/foundations-of-machine-learning-second-edition/)
        - [Free PDF (Author's Website)](https://cs.nyu.edu/~mohri/mlbook/)
        - [Amazon](https://www.amazon.com/Foundations-Machine-Learning-Adaptive-Computation/dp/0262039400)

???+ note "üî¨ Reinforcement Learning"

    23. **Sutton, R. S., & Barto, A. G.** (2018). *Reinforcement Learning: An Introduction* (2nd Edition). MIT Press.

        - [MIT Press](https://mitpress.mit.edu/9780262039246/reinforcement-learning-second-edition/)
        - [Free Online Book](http://incompleteideas.net/book/the-book-2nd.html)
        - [Amazon](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249)

    24. **Szepesv√°ri, C.** (2010). *Algorithms for Reinforcement Learning*. Morgan & Claypool.

        - [Morgan & Claypool](https://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009)
        - [Free PDF](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf)
        - [Amazon](https://www.amazon.com/Algorithms-Reinforcement-Learning-Synthesis-Artificial/dp/1608454924)

## Online Resources

???+ note "üíª Course Materials"

    - **Course GitHub Repository**: [https://github.com/shivam2003-dev/made_for_ml](https://github.com/shivam2003-dev/made_for_ml)
    - **Course Website**: [https://shivam2003-dev.github.io/made_for_ml](https://shivam2003-dev.github.io/made_for_ml)
    - Jupyter Notebooks: Interactive implementations of algorithms
    - Code Examples: Python implementations from scratch

???+ note "üìö Additional Learning Resources"

    - **Scikit-learn Documentation**: [https://scikit-learn.org/](https://scikit-learn.org/)
    - **TensorFlow Tutorials**: [https://www.tensorflow.org/learn](https://www.tensorflow.org/learn)
    - **PyTorch Tutorials**: [https://pytorch.org/tutorials/](https://pytorch.org/tutorials/)
    - **Kaggle Learn**: [https://www.kaggle.com/learn](https://www.kaggle.com/learn)
    - **Fast.ai**: [https://www.fast.ai/](https://www.fast.ai/)
    - **Andrew Ng's Machine Learning Course**: [https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning)

???+ note "üìê Mathematical Foundations"

    - **Linear Algebra**: Strang, G. (2016). *Introduction to Linear Algebra* (5th Edition). Wellesley-Cambridge Press.
      - [MIT OpenCourseWare](https://ocw.mit.edu/courses/18-06-linear-algebra-spring-2010/)
      - [Amazon](https://www.amazon.com/Introduction-Linear-Algebra-Gilbert-Strang/dp/0980232775)

    - **Probability Theory**: Ross, S. M. (2014). *A First Course in Probability* (9th Edition). Pearson.
      - [Pearson](https://www.pearson.com/en-us/subject-catalog/p/a-first-course-in-probability/P200000003431)
      - [Amazon](https://www.amazon.com/First-Course-Probability-9th/dp/032179477X)

    - **Optimization**: Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press.
      - [Free PDF (Author's Website)](https://web.stanford.edu/~boyd/cvxbook/)
      - [Cambridge University Press](https://www.cambridge.org/core/books/convex-optimization/4E2441464410C231773A45BC5F5C52FA)
      - [Amazon](https://www.amazon.com/Convex-Optimization-Stephen-Boyd/dp/0521833787)

???+ note "üîç Research Databases"

    - **Google Scholar**: [https://scholar.google.com/](https://scholar.google.com/)
    - **arXiv**: [https://arxiv.org/](https://arxiv.org/) (Preprints in ML, AI, Statistics)
    - **IEEE Xplore**: [https://ieeexplore.ieee.org/](https://ieeexplore.ieee.org/)
    - **ACM Digital Library**: [https://dl.acm.org/](https://dl.acm.org/)
    - **SpringerLink**: [https://link.springer.com/](https://link.springer.com/)

## Citation Format

???+ note "üìù Citation Guidelines"

    When citing these references in your work, use standard academic citation formats (APA, IEEE, etc.). For example:

    **APA Style**:
    Mitchell, T. M. (1997). *Machine Learning*. The McGraw-Hill Companies, Inc.

    **IEEE Style**:
    T. M. Mitchell, *Machine Learning*. New York, NY, USA: McGraw-Hill, 1997.

    **BibTeX Example**:
    ```bibtex
    @book{mitchell1997machine,
      title={Machine Learning},
      author={Mitchell, Tom M},
      year={1997},
      publisher={McGraw-Hill}
    }
    ```

---

*This reference list is not exhaustive. Additional papers and resources may be added as the course evolves.*
